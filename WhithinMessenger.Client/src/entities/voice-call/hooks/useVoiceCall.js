import { useState, useRef, useCallback, useEffect } from 'react';
import { voiceCallApi } from '../api/voiceCallApi';
import { NoiseSuppressionManager } from '../../../shared/lib/utils/noiseSuppression';
import { RoomEvent, Track } from 'livekit-client';

// üö® TEST LOGGING - –î–û–õ–ñ–ù–û –ü–û–Ø–í–ò–¢–¨–°–Ø –í –ö–û–ù–°–û–õ–ò üö®
console.log('üî•üî•üî• useVoiceCall.js LOADED üî•üî•üî•');

// ICE —Å–µ—Ä–≤–µ—Ä—ã –¥–ª—è WebRTC
const ICE_SERVERS = [
  { urls: ['stun:185.119.59.23:3478'] },
  { urls: ['stun:stun.l.google.com:19302'] },
  { urls: ['stun:stun1.l.google.com:19302'] },
  {
    urls: ['turn:185.119.59.23:3478?transport=udp'],
    username: 'test',
    credential: 'test123'
  },
  {
    urls: ['turn:185.119.59.23:3478?transport=tcp'],
    username: 'test',
    credential: 'test123'
  }
];

export const useVoiceCall = (userId, userName) => {
  const [isConnected, setIsConnected] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeaking] = useState(false);
  const [participants, setParticipants] = useState([]);
  const [volume, setVolume] = useState(1.0);
  const [audioBlocked, setAudioBlocked] = useState(false);
  const [error, setError] = useState(null);
  const [isNoiseSuppressed, setIsNoiseSuppressed] = useState(() => {
    const saved = localStorage.getItem('noiseSuppression');
    return saved ? JSON.parse(saved) : false;
  });
  const [noiseSuppressionMode, setNoiseSuppressionMode] = useState('rnnoise');
  const [userVolumes, setUserVolumes] = useState(new Map()); // –ì—Ä–æ–º–∫–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const [userMutedStates, setUserMutedStates] = useState(new Map()); // –°–æ—Å—Ç–æ—è–Ω–∏–µ –º—É—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const [showVolumeSliders, setShowVolumeSliders] = useState(new Map()); // –ü–æ–∫–∞–∑–∞—Ç—å —Å–ª–∞–π–¥–µ—Ä –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const [isGlobalAudioMuted, setIsGlobalAudioMuted] = useState(false); // –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∑–≤—É–∫–∞
  const [currentCall, setCurrentCall] = useState(null); // –¢–µ–∫—É—â–∏–π –∞–∫—Ç–∏–≤–Ω—ã–π –∑–≤–æ–Ω–æ–∫
  const [isScreenSharing, setIsScreenSharing] = useState(false); // –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–∫—Ä–∞–Ω–∞
  const [screenShareStream, setScreenShareStream] = useState(null); // –ü–æ—Ç–æ–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —ç–∫—Ä–∞–Ω–∞

  // LiveKit doesn't need mediasoup refs
  const localStreamRef = useRef(null);
  const noiseSuppressionRef = useRef(null);
  const audioContextRef = useRef(null);
  const screenShareStreamRef = useRef(null);
  const connectingRef = useRef(false);
  const gainNodesRef = useRef(new Map()); // GainNode –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const audioElementsRef = useRef(new Map()); // Audio elements –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const previousVolumesRef = useRef(new Map()); // –ü—Ä–µ–¥—ã–¥—É—â–∞—è –≥—Ä–æ–º–∫–æ—Å—Ç—å –ø–µ—Ä–µ–¥ –º—É—Ç–æ–º
  const peerIdToUserIdMapRef = useRef(new Map()); // –ú–∞–ø–ø–∏–Ω–≥ producerSocketId -> userId

  // –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–µ—Ä–≤–µ—Ä—É
  const connect = useCallback(async () => {
    try {
      // –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
      if (connectingRef.current) {
        console.log('Connection already in progress, skipping');
        return;
      }
      
      connectingRef.current = true;
      console.log('connect() called');
      console.trace('connect() call stack');
      setError(null);
      
      // –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –ø–µ—Ä–µ–¥ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–µ–π –Ω–æ–≤—ã—Ö
      voiceCallApi.off('peerJoined');
      voiceCallApi.off('peerLeft');
      voiceCallApi.off('peerMuteStateChanged');
      voiceCallApi.off('peerAudioStateChanged');
      voiceCallApi.off('newProducer');
      voiceCallApi.off('producerClosed');
      
      await voiceCallApi.connect(userId, userName);
      
      // –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
      voiceCallApi.on('peerJoined', (peerData) => {
        console.log('Peer joined:', peerData);
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ socketId -> userId
        // peerData.peerId - —ç—Ç–æ socket ID, peerData.userId - —ç—Ç–æ —Ä–µ–∞–ª—å–Ω—ã–π userId
        const socketId = peerData.peerId || peerData.id;
        if (socketId && peerData.userId) {
          peerIdToUserIdMapRef.current.set(socketId, peerData.userId);
          console.log('Updated peer mapping:', socketId, '->', peerData.userId);
          console.log('Current mapping:', Array.from(peerIdToUserIdMapRef.current.entries()));
        }
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞
        setParticipants(prev => {
          const exists = prev.some(p => p.userId === peerData.userId);
          if (exists) {
            console.log('Participant already exists, skipping:', peerData.userId);
            return prev;
          }
          return [...prev, {
          userId: peerData.userId,
          peerId: socketId, // Socket ID
          name: peerData.name,
          isMuted: peerData.isMuted || false,
          isAudioEnabled: peerData.isAudioEnabled !== undefined ? peerData.isAudioEnabled : true,
          isSpeaking: false
          }];
        });
      });

      voiceCallApi.on('peerLeft', (peerData) => {
        console.log('Peer left:', peerData);
        const socketId = peerData.peerId || peerData.id;
        
        // –ü–æ–ª—É—á–∞–µ–º userId –∏–∑ –º–∞–ø–ø–∏–Ω–≥–∞, —Ç.–∫. –≤ peerData –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å userId
        const userId = peerData.userId || peerIdToUserIdMapRef.current.get(socketId);
        
        console.log('Peer left - socketId:', socketId, 'userId:', userId);
        
        if (!userId) {
          console.warn('Cannot cleanup peer: userId not found for socketId:', socketId);
          // –í—Å–µ —Ä–∞–≤–Ω–æ —É–¥–∞–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥
          if (socketId) {
            peerIdToUserIdMapRef.current.delete(socketId);
          }
          return;
        }
        
        // –û—á–∏—â–∞–µ–º audio element
        const audioElement = audioElementsRef.current.get(userId);
        if (audioElement) {
          console.log('Removing audio element for user:', userId);
          audioElement.pause();
          audioElement.srcObject = null;
          if (audioElement.parentNode) {
            audioElement.parentNode.removeChild(audioElement);
          }
          audioElementsRef.current.delete(userId);
        }
        
        // –û—á–∏—â–∞–µ–º gain node
        const gainNode = gainNodesRef.current.get(userId);
        if (gainNode) {
          console.log('Disconnecting gain node for user:', userId);
          try {
            gainNode.disconnect();
          } catch (e) {
            console.warn('Error disconnecting gain node:', e);
          }
          gainNodesRef.current.delete(userId);
        }
        
        // –û—á–∏—â–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –∏ –º—É—Ç–∞
        setUserVolumes(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        setUserMutedStates(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        setShowVolumeSliders(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        // –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
        previousVolumesRef.current.delete(userId);
        
        // –£–¥–∞–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥ socketId -> userId
        if (socketId) {
          peerIdToUserIdMapRef.current.delete(socketId);
          console.log('Removed peer mapping for:', socketId);
        }
        
        // –£–¥–∞–ª—è–µ–º —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∏–∑ —Å–ø–∏—Å–∫–∞
        setParticipants(prev => {
          const filtered = prev.filter(p => p.userId !== userId);
          console.log('Participants after removal:', filtered);
          return filtered;
        });
        
        console.log('Peer cleanup completed for:', userId);
      });

      voiceCallApi.on('peerMuteStateChanged', ({ peerId, isMuted }) => {
        console.log('Peer mute state changed:', { peerId, isMuted });
        // peerId –∑–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å socketId, –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ userId
        const userId = peerIdToUserIdMapRef.current.get(peerId) || peerId;
        
        setParticipants(prev => prev.map(p => 
          p.userId === userId ? { ...p, isMuted: Boolean(isMuted), isSpeaking: isMuted ? false : p.isSpeaking } : p
        ));
      });

      voiceCallApi.on('peerAudioStateChanged', (data) => {
        console.log('Peer audio state changed - RAW DATA:', data);
        const { peerId, isAudioEnabled, isEnabled } = data;
        // –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –æ–±–æ–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤: isAudioEnabled –∏ isEnabled
        const audioEnabled = isAudioEnabled !== undefined ? isAudioEnabled : isEnabled;
        console.log('Peer audio state changed:', { peerId, audioEnabled, isAudioEnabled, isEnabled });
        
        // peerId –∑–¥–µ—Å—å –º–æ–∂–µ—Ç –±—ã—Ç—å socketId, –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ userId
        const userId = peerIdToUserIdMapRef.current.get(peerId) || peerId;
        console.log('Mapping peerId to userId:', { peerId, userId });
        
        setParticipants(prev => {
          const updated = prev.map(p => {
            if (p.userId === userId) {
              console.log(`Updating participant ${p.userId} audio state to:`, audioEnabled);
              return { ...p, isAudioEnabled: Boolean(audioEnabled) };
            }
            return p;
          });
          console.log('Updated participants:', updated);
          return updated;
        });
      });

      // Handle TrackSubscribed events from LiveKit
      voiceCallApi.on('trackSubscribed', async ({ track, publication, participant, userId, mediaType }) => {
        console.log('üîä Track subscribed event received:', { 
          trackSid: track?.sid, 
          kind: track?.kind, 
          userId, 
          mediaType,
          hasMediaStreamTrack: !!track?.mediaStreamTrack,
          trackState: track?.mediaStreamTrack?.readyState
        });
        
        // Only handle audio tracks for voice calls
        if (track.kind !== 'audio') {
          console.log('Skipping non-audio track');
          return;
        }
        
        // Skip screen share audio (handled separately if needed)
        if (mediaType === 'screen') {
          console.log('Screen share audio track, skipping standard audio processing');
          return;
        }
        
        // Check if track has mediaStreamTrack
        if (!track.mediaStreamTrack) {
          console.error('Track has no mediaStreamTrack!', track);
          return;
        }
        
        // Use userId from event or fallback to participant identity
        const targetUserId = userId || participant.identity;
        console.log('üîä Processing audio track for userId:', targetUserId);
        
        // Check if we already have an audio element for this user
        if (audioElementsRef.current.has(targetUserId)) {
          console.log('üîä Audio element already exists for user:', targetUserId, 'updating...');
          const existingElement = audioElementsRef.current.get(targetUserId);
          existingElement.srcObject = new MediaStream([track.mediaStreamTrack]);
          try {
            await existingElement.play();
            console.log('üîä Updated audio element playback started');
          } catch (error) {
            console.warn('üîä Failed to play updated audio element:', error);
          }
          return;
        }
        
        // Initialize AudioContext if needed
        if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
          audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 48000,
            latencyHint: 'interactive'
          });
          console.log('üîä Created new AudioContext');
        }
        
        // Resume audio context if suspended
        if (audioContextRef.current.state === 'suspended') {
          await audioContextRef.current.resume();
          console.log('üîä Resumed AudioContext');
        }
        
        // Create audio element
        const audioElement = document.createElement('audio');
        const mediaStream = new MediaStream([track.mediaStreamTrack]);
        audioElement.srcObject = mediaStream;
        audioElement.autoplay = true;
        audioElement.playsInline = true;
        audioElement.controls = false;
        audioElement.style.display = 'none';
        document.body.appendChild(audioElement);
        console.log('üîä Created audio element for user:', targetUserId);
        
        // Create Web Audio API chain: source -> gain -> destination
        const source = audioContextRef.current.createMediaStreamSource(mediaStream);
        const gainNode = audioContextRef.current.createGain();
        
        // Set initial volume
        const initialVolume = userVolumes.get(targetUserId) || 100;
        const isMuted = userMutedStates.get(targetUserId) || false;
        const audioVolume = isGlobalAudioMuted ? 0 : (isMuted ? 0 : (initialVolume / 100.0));
        audioElement.volume = audioVolume;
        gainNode.gain.value = audioVolume;
        
        // Connect source -> gain (not to destination to avoid double playback with audio element)
        source.connect(gainNode);
        console.log('üîä Connected Web Audio API chain (gain node for volume control)');
        
        // Save references
        gainNodesRef.current.set(targetUserId, gainNode);
        audioElementsRef.current.set(targetUserId, audioElement);
        
        // Initialize volume in state if not set
        if (!userVolumes.has(targetUserId)) {
          setUserVolumes(prev => {
            const newMap = new Map(prev);
            newMap.set(targetUserId, 100);
            return newMap;
          });
        }
        
        // Try to play audio element
        try {
          await audioElement.play();
          console.log('üîä‚úÖ Audio playback started for peer:', targetUserId);
          setAudioBlocked(false);
        } catch (error) {
          console.warn('üîä‚ö†Ô∏è Auto-play blocked, user interaction required:', error);
          setAudioBlocked(true);
          // Try again after a delay
          setTimeout(async () => {
            try {
              await audioElement.play();
              console.log('üîä‚úÖ Audio playback started after delay');
              setAudioBlocked(false);
            } catch (err) {
              console.error('üîä‚ùå Audio playback still blocked:', err);
            }
          }, 1000);
        }
      });
      
      // Keep newProducer for compatibility (but it won't be used with LiveKit)
      voiceCallApi.on('newProducer', async (producerData) => {
        console.log('New producer event received (legacy):', producerData);
        // This is handled by trackSubscribed now
      });

      // Handle producerClosed (track unpublished) for cleanup
      voiceCallApi.on('producerClosed', (data) => {
        console.log('Producer closed (track unpublished):', data);
        const producerSocketId = data.producerSocketId || data.participantIdentity;
        
        // If we have producerSocketId, get userId from mapping
        if (producerSocketId) {
          const userId = peerIdToUserIdMapRef.current.get(producerSocketId) || producerSocketId;
          console.log('Producer closed for socketId:', producerSocketId, 'userId:', userId);
          
          // Only cleanup if it's an audio track (video tracks are handled separately)
          if (data.kind === 'audio' && data.mediaType !== 'screen') {
            // –û—á–∏—â–∞–µ–º audio element
            const audioElement = audioElementsRef.current.get(userId);
            if (audioElement) {
              console.log('Removing audio element for user:', userId);
              audioElement.pause();
              audioElement.srcObject = null;
              if (audioElement.parentNode) {
                audioElement.parentNode.removeChild(audioElement);
              }
              audioElementsRef.current.delete(userId);
            }
            
            // –û—á–∏—â–∞–µ–º gain node
            const gainNode = gainNodesRef.current.get(userId);
            if (gainNode) {
              console.log('Disconnecting gain node for user:', userId);
              try {
                gainNode.disconnect();
              } catch (e) {
                console.warn('Error disconnecting gain node:', e);
              }
              gainNodesRef.current.delete(userId);
            }
            
            // –û—á–∏—â–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
            setUserVolumes(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            setUserMutedStates(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            setShowVolumeSliders(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            previousVolumesRef.current.delete(userId);
            console.log('Audio cleanup completed for userId:', userId);
          }
        }
      });
      
      setIsConnected(true);
      connectingRef.current = false;
      
      // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω–∞ —Å–µ—Ä–≤–µ—Ä
      if (voiceCallApi.socket) {
        voiceCallApi.socket.emit('muteState', { isMuted: isMuted });
        voiceCallApi.socket.emit('audioState', { isEnabled: !isGlobalAudioMuted });
        console.log('Initial states sent to server - muted:', isMuted, 'audio (headphones on):', !isGlobalAudioMuted);
      }
    } catch (error) {
      console.error('Failed to connect to voice server:', error);
      setError(error.message);
      connectingRef.current = false;
    }
  }, [userId, userName, isMuted, isGlobalAudioMuted]);

  // –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞
  const disconnect = useCallback(async () => {
    try {
      console.log('Disconnecting from voice call...');
      
      // –û—á–∏—â–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
      voiceCallApi.off('peerJoined');
      voiceCallApi.off('peerLeft');
      voiceCallApi.off('peerMuteStateChanged');
      voiceCallApi.off('peerAudioStateChanged');
      voiceCallApi.off('newProducer');
      voiceCallApi.off('producerClosed');
      voiceCallApi.off('trackSubscribed');
      console.log('Event handlers cleared');
      
      if (localStreamRef.current) {
        localStreamRef.current.getTracks().forEach(track => track.stop());
        localStreamRef.current = null;
      }
      
      // –û—á–∏—Å—Ç–∫–∞ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è
      if (noiseSuppressionRef.current) {
        noiseSuppressionRef.current.cleanup();
        noiseSuppressionRef.current = null;
      }
      
      // –ó–∞–∫—Ä—ã—Ç–∏–µ audio context
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        await audioContextRef.current.close();
        audioContextRef.current = null;
      }
      
      // LiveKit handles cleanup automatically
      
      // –û—á–∏—Å—Ç–∫–∞ GainNodes –∏ audio elements
      gainNodesRef.current.forEach(gainNode => {
        try {
          gainNode.disconnect();
        } catch (e) {
          console.warn('Error disconnecting gain node:', e);
        }
      });
      gainNodesRef.current.clear();
      
      audioElementsRef.current.forEach(audioElement => {
        try {
          audioElement.pause();
          audioElement.srcObject = null;
          if (audioElement.parentNode) {
            audioElement.parentNode.removeChild(audioElement);
          }
        } catch (e) {
          console.warn('Error removing audio element:', e);
        }
      });
      audioElementsRef.current.clear();
      previousVolumesRef.current.clear();
      peerIdToUserIdMapRef.current.clear();
      
      await voiceCallApi.disconnect();
      setIsConnected(false);
      setCurrentCall(null); // –û—á–∏—â–∞–µ–º —Ç–µ–∫—É—â–∏–π –∑–≤–æ–Ω–æ–∫
      setParticipants([]);
      setUserVolumes(new Map());
      setUserMutedStates(new Map());
      setShowVolumeSliders(new Map());
      connectingRef.current = false;
      console.log('Disconnected from voice server');
    } catch (error) {
      console.error('Failed to disconnect:', error);
      connectingRef.current = false;
    }
  }, []);

  // LiveKit handles audio/video tracks automatically, no need for mediasoup transports/producers/consumers

  // –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∫ –∫–æ–º–Ω–∞—Ç–µ
  const joinRoom = useCallback(async (roomId, initialMuted = false) => {
    try {
      console.log('joinRoom called for roomId:', roomId);
      const response = await voiceCallApi.joinRoom(roomId, userName, userId, initialMuted);
      
      if (response.existingPeers) {
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥ –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∏—Ä–æ–≤
        response.existingPeers.forEach(peer => {
          const socketId = peer.peerId || peer.id;
          if (socketId && peer.userId) {
            peerIdToUserIdMapRef.current.set(socketId, peer.userId);
            console.log('Updated existing peer mapping:', socketId, '->', peer.userId);
          }
        });
        
        setParticipants(response.existingPeers.map(peer => ({
          userId: peer.userId,
          peerId: peer.peerId || peer.id,
          name: peer.name,
          isMuted: peer.isMuted || false,
          isAudioEnabled: peer.isAudioEnabled !== undefined ? peer.isAudioEnabled : true,
          isSpeaking: false
        })));
      }
      
      // Get room and set initial mute state
      const room = voiceCallApi.getRoom();
      if (room) {
        // Set initial microphone state
        await room.localParticipant.setMicrophoneEnabled(!initialMuted);
        setIsMuted(initialMuted);
      }
      
      // –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—É—â–∏–π –∑–≤–æ–Ω–æ–∫
      setCurrentCall({ channelId: roomId, channelName: roomId });
      console.log('Current call set to:', { channelId: roomId, channelName: roomId });
    } catch (error) {
      console.error('Failed to join room:', error);
      setError(error.message);
    }
  }, [userName, userId]);

  // –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
  const toggleMute = useCallback(async () => {
    const newMutedState = !isMuted;
    
    try {
      // Use LiveKit API to toggle microphone
      await voiceCallApi.setMicrophoneEnabled(!newMutedState);
      setIsMuted(newMutedState);
    } catch (error) {
      console.error('Error toggling mute:', error);
      setError(error.message);
    }
  }, [isMuted]);


  // –ò–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
  const handleVolumeChange = useCallback((newVolume) => {
    setVolume(newVolume);
    const audioElements = document.querySelectorAll('audio');
    audioElements.forEach(audio => {
      audio.volume = newVolume;
    });
  }, []);

  // –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º—É—Ç–∞ –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const toggleUserMute = useCallback((peerId) => {
    console.log('toggleUserMute called for:', peerId);
    console.log('Available audio elements:', Array.from(audioElementsRef.current.keys()));
    const audioElement = audioElementsRef.current.get(peerId);
    const gainNode = gainNodesRef.current.get(peerId);
    if (!audioElement) {
      console.error('Audio element not found for peer:', peerId);
      return;
    }

    const isCurrentlyMuted = userMutedStates.get(peerId) || false;
    const newIsMuted = !isCurrentlyMuted;

    if (newIsMuted) {
      // –ú—É—Ç–∏–º - —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º 0, –ù–ï –º–µ–Ω—è—è –ø–æ–ª–∑—É–Ω–æ–∫ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
      const currentVolume = userVolumes.get(peerId) || 100;
      previousVolumesRef.current.set(peerId, currentVolume);
      audioElement.volume = 0;
      if (gainNode) {
        gainNode.gain.value = 0;
      }
      // –ù–ï –º–µ–Ω—è–µ–º userVolumes, —á—Ç–æ–±—ã –ø–æ–ª–∑—É–Ω–æ–∫ –æ—Å—Ç–∞–ª—Å—è –Ω–∞ –º–µ—Å—Ç–µ
    } else {
      // –†–∞–∑–º—É—Ç–∏–≤–∞–µ–º - –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–≤—É–∫ –Ω–∞ —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é –ø–æ–ª–∑—É–Ω–∫–∞
      const currentVolume = userVolumes.get(peerId) || 100;
      const audioVolume = isGlobalAudioMuted ? 0 : (currentVolume / 100.0);
      audioElement.volume = audioVolume;
      if (gainNode) {
        gainNode.gain.value = audioVolume;
      }
      // –ù–ï –º–µ–Ω—è–µ–º userVolumes, –ø–æ–ª–∑—É–Ω–æ–∫ —É–∂–µ –Ω–∞ –Ω—É–∂–Ω–æ–º –º–µ—Å—Ç–µ
    }

    setUserMutedStates(prev => {
      const newMap = new Map(prev);
      newMap.set(peerId, newIsMuted);
      return newMap;
    });

    console.log(`User ${peerId} ${newIsMuted ? 'muted' : 'unmuted'}`);
  }, [userVolumes, userMutedStates, isGlobalAudioMuted]);

  // –ò–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
  const changeUserVolume = useCallback((peerId, newVolume) => {
    console.log('changeUserVolume called for:', peerId, 'newVolume:', newVolume);
    console.log('Available audio elements:', Array.from(audioElementsRef.current.keys()));
    const audioElement = audioElementsRef.current.get(peerId);
    if (!audioElement) {
      console.error('Audio element not found for peer:', peerId);
      return;
    }

    // –ï—Å–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ –∑–∞–º—É—á–µ–Ω–æ, –ø—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–ª—å–∫–æ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, –Ω–æ –Ω–µ –∑–≤—É–∫
    const audioVolume = isGlobalAudioMuted ? 0 : (newVolume / 100.0);
    audioElement.volume = audioVolume;

    setUserVolumes(prev => {
      const newMap = new Map(prev);
      newMap.set(peerId, newVolume);
      return newMap;
    });

    // –ï—Å–ª–∏ —Ä–∞–∑–º—É—Ç–∏–≤–∞–µ–º —á–µ—Ä–µ–∑ —Å–ª–∞–π–¥–µ—Ä, –æ–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º—É—Ç–∞
    if (newVolume > 0 && userMutedStates.get(peerId)) {
      setUserMutedStates(prev => {
        const newMap = new Map(prev);
        newMap.set(peerId, false);
        return newMap;
      });
    } else if (newVolume === 0 && !userMutedStates.get(peerId)) {
      setUserMutedStates(prev => {
        const newMap = new Map(prev);
        newMap.set(peerId, true);
        return newMap;
      });
    }

    console.log(`User ${peerId} volume set to ${newVolume}%`);
  }, [userMutedStates, isGlobalAudioMuted]);

  // –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å–ª–∞–π–¥–µ—Ä–∞ –≥—Ä–æ–º–∫–æ—Å—Ç–∏
  const toggleVolumeSlider = useCallback((peerId) => {
    setShowVolumeSliders(prev => {
      const newMap = new Map(prev);
      const currentState = newMap.get(peerId) || false;
      newMap.set(peerId, !currentState);
      return newMap;
    });
  }, []);

  // –ì–ª–æ–±–∞–ª—å–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ/–≤–∫–ª—é—á–µ–Ω–∏–µ –∑–≤—É–∫–∞ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ (–Ω–∞—É—à–Ω–∏–∫–∏)
  const toggleGlobalAudio = useCallback(() => {
    const newMutedState = !isGlobalAudioMuted;
    
    console.log(`toggleGlobalAudio called, new state: ${newMutedState}`);
    console.log(`Audio elements count: ${audioElementsRef.current.size}`);
    
    // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–∞—É—à–Ω–∏–∫–æ–≤ –Ω–∞ —Å–µ—Ä–≤–µ—Ä
    // isGlobalAudioMuted=true –æ–∑–Ω–∞—á–∞–µ—Ç isAudioEnabled=false
    if (voiceCallApi.socket) {
      voiceCallApi.socket.emit('audioState', { isEnabled: !newMutedState });
      console.log('Audio state (headphones) sent to server, isEnabled:', !newMutedState);
    }
    
    // –£–ø—Ä–∞–≤–ª—è–µ–º HTML Audio —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –∏ GainNodes
    audioElementsRef.current.forEach((audioElement, peerId) => {
      const gainNode = gainNodesRef.current.get(peerId);
      if (audioElement) {
        if (newMutedState) {
          // –ú—É—Ç–∏–º HTML Audio —ç–ª–µ–º–µ–Ω—Ç –∏ gainNode
          audioElement.volume = 0;
          if (gainNode) {
            gainNode.gain.value = 0;
          }
          console.log(`HTML Audio muted for peer: ${peerId}`);
        } else {
          // –†–∞–∑–º—É—Ç–∏–≤–∞–µ–º —Å –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ–π –≥—Ä–æ–º–∫–æ—Å—Ç—å—é
          const volume = userVolumes.get(peerId) || 100;
          const isIndividuallyMuted = userMutedStates.get(peerId) || false;
          const audioVolume = isIndividuallyMuted ? 0 : (volume / 100.0);
          audioElement.volume = audioVolume;
          if (gainNode) {
            gainNode.gain.value = audioVolume;
          }
          console.log(`HTML Audio unmuted for peer: ${peerId}, volume: ${audioVolume}`);
        }
      }
    });
    
    setIsGlobalAudioMuted(newMutedState);
    console.log(`Global audio ${newMutedState ? 'muted' : 'unmuted'}`);
  }, [isGlobalAudioMuted, userVolumes, userMutedStates]);

  // –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è
  const toggleNoiseSuppression = useCallback(async () => {
    try {
      if (!noiseSuppressionRef.current || !noiseSuppressionRef.current.isInitialized()) {
        console.error('Noise suppression not initialized');
        return false;
      }

      const newState = !isNoiseSuppressed;
      console.log(`Toggling noise suppression: ${isNoiseSuppressed} -> ${newState}`);
      
      let success = false;

      if (newState) {
        success = await noiseSuppressionRef.current.enable(noiseSuppressionMode);
        console.log(`Noise suppression enabled with mode: ${noiseSuppressionMode}, success: ${success}`);
      } else {
        success = await noiseSuppressionRef.current.disable();
        console.log(`Noise suppression disabled, success: ${success}`);
      }

      if (success) {
        setIsNoiseSuppressed(newState);
        localStorage.setItem('noiseSuppression', JSON.stringify(newState));
        
        // –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–∫–∞
        if (noiseSuppressionRef.current) {
          const processedStream = noiseSuppressionRef.current.getProcessedStream();
          const audioTrack = processedStream?.getAudioTracks()[0];
          if (audioTrack) {
            audioTrack.enabled = !isMuted;
            console.log(`Updated audio track enabled state: ${audioTrack.enabled}`);
          }
        }
        
        console.log('Noise suppression ' + (newState ? 'enabled' : 'disabled') + ' successfully');
        return true;
      } else {
        console.error('Failed to toggle noise suppression');
        return false;
      }
    } catch (error) {
      console.error('Error toggling noise suppression:', error);
      return false;
    }
  }, [isNoiseSuppressed, noiseSuppressionMode, isMuted]);

  // –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è
  const changeNoiseSuppressionMode = useCallback(async (mode) => {
    try {
      if (!noiseSuppressionRef.current || !noiseSuppressionRef.current.isInitialized()) {
        console.error('Noise suppression not initialized');
        return false;
      }

      console.log(`Changing noise suppression mode from ${noiseSuppressionMode} to ${mode}`);
      
      let success = false;

      if (!isNoiseSuppressed) {
        success = await noiseSuppressionRef.current.enable(mode);
        console.log(`Enabled noise suppression with mode: ${mode}, success: ${success}`);
      } else if (mode !== noiseSuppressionMode) {
        // –ï—Å–ª–∏ –º–µ–Ω—è–µ–º —Ä–µ–∂–∏–º –ø—Ä–∏ –≤–∫–ª—é—á–µ–Ω–Ω–æ–º —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–∏
        success = await noiseSuppressionRef.current.enable(mode);
        console.log(`Changed mode to: ${mode}, success: ${success}`);
      } else {
        console.log(`Mode ${mode} is already active`);
        return true; // –†–µ–∂–∏–º —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
      }

      if (success) {
        setNoiseSuppressionMode(mode);
        setIsNoiseSuppressed(true);
        localStorage.setItem('noiseSuppression', JSON.stringify(true));
        
        // –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç—Ä–µ–∫–∞
        if (noiseSuppressionRef.current) {
          const processedStream = noiseSuppressionRef.current.getProcessedStream();
          const audioTrack = processedStream?.getAudioTracks()[0];
          if (audioTrack) {
            audioTrack.enabled = !isMuted;
            console.log(`Updated audio track enabled state after mode change: ${audioTrack.enabled}`);
          }
        }
        
        console.log('Noise suppression mode changed to:', mode);
        return true;
      }
      
      return false;
    } catch (error) {
      console.error('Error changing noise suppression mode:', error);
      return false;
    }
  }, [isNoiseSuppressed, noiseSuppressionMode, isMuted]);

  // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–∫ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –∏–∑ –¥—Ä—É–≥–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
  useEffect(() => {
    const handleNoiseSuppressionChanged = (event) => {
      const { enabled } = event.detail;
      setIsNoiseSuppressed(enabled);
      
      // –ï—Å–ª–∏ —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ –∏ —É –Ω–∞—Å –µ—Å—Ç—å –ø–æ—Ç–æ–∫, –≤–∫–ª—é—á–∞–µ–º –µ–≥–æ
      if (enabled && noiseSuppressionRef.current) {
        noiseSuppressionRef.current.enable(noiseSuppressionMode);
      } else if (!enabled && noiseSuppressionRef.current) {
        noiseSuppressionRef.current.disable();
      }
    };

    window.addEventListener('noiseSuppressionChanged', handleNoiseSuppressionChanged);
    return () => {
      window.removeEventListener('noiseSuppressionChanged', handleNoiseSuppressionChanged);
    };
  }, [noiseSuppressionMode]);

  // –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —ç–∫—Ä–∞–Ω–∞
  const startScreenShare = useCallback(async () => {
    try {
      console.log('Starting screen share...');
      
      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é —ç–∫—Ä–∞–Ω–∞, –µ—Å–ª–∏ –µ—Å—Ç—å
      if (isScreenSharing) {
        await stopScreenShare();
      }

      // Use LiveKit API to start screen share
      await voiceCallApi.setScreenShareEnabled(true);
      
      // Get the screen share stream from LiveKit room
      const room = voiceCallApi.getRoom();
      if (room) {
        // Listen for local track published event to get the stream
        const handleLocalTrackPublished = (publication) => {
          // Check if it's a screen share track
          const isScreenShare = publication.source === Track.Source.ScreenShare || 
                               publication.source === 'screen_share';
          
          if (isScreenShare && publication.track) {
            const stream = new MediaStream([publication.track.mediaStreamTrack]);
            setScreenShareStream(stream);
            screenShareStreamRef.current = stream;
            
            // Handle track ended
            publication.track.on('ended', () => {
              console.log('Screen sharing stopped by user');
              stopScreenShare();
            });
          }
        };
        
        // Check if screen share track already exists
        room.localParticipant.videoTrackPublications.forEach(publication => {
          const isScreenShare = publication.source === Track.Source.ScreenShare || 
                               publication.source === 'screen_share';
          if (isScreenShare && publication.track) {
            handleLocalTrackPublished(publication);
          }
        });
        
        // Listen for new screen share tracks
        room.on(RoomEvent.LocalTrackPublished, handleLocalTrackPublished);
      }
      
      setIsScreenSharing(true);
    } catch (error) {
      console.error('Error starting screen share:', error);
      
      // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—Ç–º–µ–Ω–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
      const isCancelled = error.message && (
        error.message.includes('–æ—Ç–º–µ–Ω–µ–Ω–∞') || 
        error.message.includes('cancelled') ||
        error.message.includes('canceled') ||
        error.message.includes('Permission denied') ||
        error.name === 'NotAllowedError' ||
        error.name === 'AbortError'
      );
      
      setIsScreenSharing(false);
      
      // –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—à–∏–±–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ –æ—Ç–º–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
      if (!isCancelled) {
        setError('Failed to start screen sharing: ' + error.message);
      } else {
        console.log('Screen sharing cancelled by user');
      }
    }
  }, [isScreenSharing, stopScreenShare]);

  const stopScreenShare = useCallback(async () => {
    console.log('Stopping screen sharing...');

    try {
      // Use LiveKit API to stop screen share
      await voiceCallApi.setScreenShareEnabled(false);

      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ç–æ–∫
      if (screenShareStreamRef.current) {
        screenShareStreamRef.current.getTracks().forEach(track => track.stop());
      }

      // –û—á–∏—â–∞–µ–º audio elements –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —ç–∫—Ä–∞–Ω–∞
      const screenShareAudioKey = `screen-share-${userId}`;
      const screenShareAudioElement = audioElementsRef.current.get(screenShareAudioKey);
      if (screenShareAudioElement) {
        console.log('Removing screen share audio element for user:', userId);
        screenShareAudioElement.pause();
        screenShareAudioElement.srcObject = null;
        if (screenShareAudioElement.parentNode) {
          screenShareAudioElement.parentNode.removeChild(screenShareAudioElement);
        }
        audioElementsRef.current.delete(screenShareAudioKey);
      }

      // –û—á–∏—â–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
      setScreenShareStream(null);
      screenShareStreamRef.current = null;
      setIsScreenSharing(false);

      console.log('Screen sharing stopped successfully');
    } catch (error) {
      console.error('Error stopping screen share:', error);
      setError('Failed to stop screen sharing: ' + error.message);
    }
  }, [userId]);

  const toggleScreenShare = useCallback(async () => {
    if (isScreenSharing) {
      await stopScreenShare();
    } else {
      await startScreenShare();
    }
  }, [isScreenSharing, startScreenShare, stopScreenShare]);

  return {
    // –°–æ—Å—Ç–æ—è–Ω–∏–µ
    isConnected,
    isMuted,
    isAudioEnabled: !isGlobalAudioMuted, // –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: isAudioEnabled –æ–∑–Ω–∞—á–∞–µ—Ç "–Ω–∞—É—à–Ω–∏–∫–∏ –≤–∫–ª—é—á–µ–Ω—ã"
    isSpeaking,
    participants,
    volume,
    audioBlocked,
    error,
    isNoiseSuppressed,
    noiseSuppressionMode,
    userVolumes,
    userMutedStates,
    showVolumeSliders,
    isGlobalAudioMuted,
    currentCall,
    isScreenSharing,
    screenShareStream,
    
    // –ú–µ—Ç–æ–¥—ã
    connect,
    disconnect,
    joinRoom,
    toggleMute,
    handleVolumeChange,
    toggleNoiseSuppression,
    changeNoiseSuppressionMode,
    toggleUserMute,
    changeUserVolume,
    toggleVolumeSlider,
    toggleGlobalAudio,
    startScreenShare,
    stopScreenShare,
    toggleScreenShare
  };
};