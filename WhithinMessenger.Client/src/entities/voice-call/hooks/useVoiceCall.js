import { useState, useRef, useCallback, useEffect } from 'react';
import { voiceCallApi } from '../api/voiceCallApi';
import { NoiseSuppressionManager } from '../../../shared/lib/utils/noiseSuppression';

// ðŸš¨ TEST LOGGING - Ð”ÐžÐ›Ð–ÐÐž ÐŸÐžÐ¯Ð’Ð˜Ð¢Ð¬Ð¡Ð¯ Ð’ ÐšÐžÐÐ¡ÐžÐ›Ð˜ ðŸš¨
console.log('ðŸ”¥ðŸ”¥ðŸ”¥ useVoiceCall.js LOADED ðŸ”¥ðŸ”¥ðŸ”¥');

// ICE ÑÐµÑ€Ð²ÐµÑ€Ñ‹ Ð´Ð»Ñ WebRTC
const ICE_SERVERS = [
  { urls: ['stun:185.119.59.23:3478'] },
  { urls: ['stun:stun.l.google.com:19302'] },
  { urls: ['stun:stun1.l.google.com:19302'] },
  {
    urls: ['turn:185.119.59.23:3478?transport=udp'],
    username: 'test',
    credential: 'test123'
  },
  {
    urls: ['turn:185.119.59.23:3478?transport=tcp'],
    username: 'test',
    credential: 'test123'
  }
];

export const useVoiceCall = (userId, userName) => {
  const [isConnected, setIsConnected] = useState(false);
  const [isMuted, setIsMuted] = useState(false);
  const [isSpeaking] = useState(false);
  const [participants, setParticipants] = useState([]);
  const [volume, setVolume] = useState(1.0);
  const [audioBlocked, setAudioBlocked] = useState(false);
  const [error, setError] = useState(null);
  const [isNoiseSuppressed, setIsNoiseSuppressed] = useState(() => {
    const saved = localStorage.getItem('noiseSuppression');
    return saved ? JSON.parse(saved) : false;
  });
  const [noiseSuppressionMode, setNoiseSuppressionMode] = useState('rnnoise');
  const [userVolumes, setUserVolumes] = useState(new Map()); // Ð“Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const [userMutedStates, setUserMutedStates] = useState(new Map()); // Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÑƒÑ‚Ð° Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const [showVolumeSliders, setShowVolumeSliders] = useState(new Map()); // ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÐ»Ð°Ð¹Ð´ÐµÑ€ Ð´Ð»Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const [isGlobalAudioMuted, setIsGlobalAudioMuted] = useState(false); // Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð²ÑƒÐºÐ°
  const [currentCall, setCurrentCall] = useState(null); // Ð¢ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð·Ð²Ð¾Ð½Ð¾Ðº
  const [isScreenSharing, setIsScreenSharing] = useState(false); // Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÐºÑ€Ð°Ð½Ð°
  const [screenShareStream, setScreenShareStream] = useState(null); // ÐŸÐ¾Ñ‚Ð¾Ðº Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°

  const deviceRef = useRef(null);
  const sendTransportRef = useRef(null);
  const recvTransportRef = useRef(null);
  const producersRef = useRef(new Map());
  const consumersRef = useRef(new Map());
  const localStreamRef = useRef(null);
  const handleNewProducerRef = useRef(null);
  const noiseSuppressionRef = useRef(null);
  const audioContextRef = useRef(null);
  const createAudioStreamRef = useRef(null);
  const screenProducerRef = useRef(null);
  const connectingRef = useRef(false);
  const gainNodesRef = useRef(new Map()); // GainNode Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const audioElementsRef = useRef(new Map()); // Audio elements Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const previousVolumesRef = useRef(new Map()); // ÐŸÑ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð°Ñ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ Ð¿ÐµÑ€ÐµÐ´ Ð¼ÑƒÑ‚Ð¾Ð¼
  const peerIdToUserIdMapRef = useRef(new Map()); // ÐœÐ°Ð¿Ð¿Ð¸Ð½Ð³ producerSocketId -> userId

  // ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ðº ÑÐµÑ€Ð²ÐµÑ€Ñƒ
  const connect = useCallback(async () => {
    try {
      // ÐŸÑ€ÐµÐ´Ð¾Ñ‚Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð¼Ð½Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ
      if (connectingRef.current) {
        console.log('Connection already in progress, skipping');
        return;
      }
      
      connectingRef.current = true;
      console.log('connect() called');
      console.trace('connect() call stack');
      setError(null);
      
      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¸ Ð¿ÐµÑ€ÐµÐ´ Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ Ð½Ð¾Ð²Ñ‹Ñ…
      voiceCallApi.off('peerJoined');
      voiceCallApi.off('peerLeft');
      voiceCallApi.off('peerMuteStateChanged');
      voiceCallApi.off('peerAudioStateChanged');
      voiceCallApi.off('newProducer');
      voiceCallApi.off('producerClosed');
      
      await voiceCallApi.connect(userId, userName);
      
      // Ð ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¸ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ Ð¡Ð ÐÐ—Ð£ Ð¿Ð¾ÑÐ»Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ
      voiceCallApi.on('peerJoined', (peerData) => {
        console.log('Peer joined:', peerData);
        // Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ socketId -> userId
        // peerData.peerId - ÑÑ‚Ð¾ socket ID, peerData.userId - ÑÑ‚Ð¾ Ñ€ÐµÐ°Ð»ÑŒÐ½Ñ‹Ð¹ userId
        const socketId = peerData.peerId || peerData.id;
        if (socketId && peerData.userId) {
          peerIdToUserIdMapRef.current.set(socketId, peerData.userId);
          console.log('Updated peer mapping:', socketId, '->', peerData.userId);
          console.log('Current mapping:', Array.from(peerIdToUserIdMapRef.current.entries()));
        }
        // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½ÐµÑ‚ Ð»Ð¸ ÑƒÐ¶Ðµ Ñ‚Ð°ÐºÐ¾Ð³Ð¾ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ°
        setParticipants(prev => {
          const exists = prev.some(p => p.userId === peerData.userId);
          if (exists) {
            console.log('Participant already exists, skipping:', peerData.userId);
            return prev;
          }
          return [...prev, {
          userId: peerData.userId,
          peerId: socketId, // Socket ID
          name: peerData.name,
          isMuted: peerData.isMuted || false,
          isAudioEnabled: peerData.isAudioEnabled !== undefined ? peerData.isAudioEnabled : true,
          isSpeaking: false
          }];
        });
      });

      voiceCallApi.on('peerLeft', (peerData) => {
        console.log('Peer left:', peerData);
        const socketId = peerData.peerId || peerData.id;
        
        // ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ userId Ð¸Ð· Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð°, Ñ‚.Ðº. Ð² peerData Ð¼Ð¾Ð¶ÐµÑ‚ Ð½Ðµ Ð±Ñ‹Ñ‚ÑŒ userId
        const userId = peerData.userId || peerIdToUserIdMapRef.current.get(socketId);
        
        console.log('Peer left - socketId:', socketId, 'userId:', userId);
        
        if (!userId) {
          console.warn('Cannot cleanup peer: userId not found for socketId:', socketId);
          // Ð’ÑÐµ Ñ€Ð°Ð²Ð½Ð¾ ÑƒÐ´Ð°Ð»ÑÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³
          if (socketId) {
            peerIdToUserIdMapRef.current.delete(socketId);
          }
          return;
        }
        
        // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ audio element
        const audioElement = audioElementsRef.current.get(userId);
        if (audioElement) {
          console.log('Removing audio element for user:', userId);
          audioElement.pause();
          audioElement.srcObject = null;
          if (audioElement.parentNode) {
            audioElement.parentNode.removeChild(audioElement);
          }
          audioElementsRef.current.delete(userId);
        }
        
        // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ gain node
        const gainNode = gainNodesRef.current.get(userId);
        if (gainNode) {
          console.log('Disconnecting gain node for user:', userId);
          try {
            gainNode.disconnect();
          } catch (e) {
            console.warn('Error disconnecting gain node:', e);
          }
          gainNodesRef.current.delete(userId);
        }
        
        // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸ Ð¸ Ð¼ÑƒÑ‚Ð°
        setUserVolumes(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        setUserMutedStates(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        setShowVolumeSliders(prev => {
          const newMap = new Map(prev);
          newMap.delete(userId);
          return newMap;
        });
        
        // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ðµ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸
        previousVolumesRef.current.delete(userId);
        
        // Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ socketId -> userId
        if (socketId) {
          peerIdToUserIdMapRef.current.delete(socketId);
          console.log('Removed peer mapping for:', socketId);
        }
        
        // Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ° Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ°
        setParticipants(prev => {
          const filtered = prev.filter(p => p.userId !== userId);
          console.log('Participants after removal:', filtered);
          return filtered;
        });
        
        console.log('Peer cleanup completed for:', userId);
      });

      voiceCallApi.on('peerMuteStateChanged', ({ peerId, isMuted }) => {
        console.log('Peer mute state changed:', { peerId, isMuted });
        // peerId Ð·Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ socketId, Ð½ÑƒÐ¶Ð½Ð¾ Ð½Ð°Ð¹Ñ‚Ð¸ userId
        const userId = peerIdToUserIdMapRef.current.get(peerId) || peerId;
        
        setParticipants(prev => prev.map(p => 
          p.userId === userId ? { ...p, isMuted: Boolean(isMuted), isSpeaking: isMuted ? false : p.isSpeaking } : p
        ));
      });

      voiceCallApi.on('peerAudioStateChanged', (data) => {
        console.log('Peer audio state changed - RAW DATA:', data);
        const { peerId, isAudioEnabled, isEnabled } = data;
        // ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ° Ð¾Ð±Ð¾Ð¸Ñ… Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¾Ð²: isAudioEnabled Ð¸ isEnabled
        const audioEnabled = isAudioEnabled !== undefined ? isAudioEnabled : isEnabled;
        console.log('Peer audio state changed:', { peerId, audioEnabled, isAudioEnabled, isEnabled });
        
        // peerId Ð·Ð´ÐµÑÑŒ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ socketId, Ð½ÑƒÐ¶Ð½Ð¾ Ð½Ð°Ð¹Ñ‚Ð¸ userId
        const userId = peerIdToUserIdMapRef.current.get(peerId) || peerId;
        console.log('Mapping peerId to userId:', { peerId, userId });
        
        setParticipants(prev => {
          const updated = prev.map(p => {
            if (p.userId === userId) {
              console.log(`Updating participant ${p.userId} audio state to:`, audioEnabled);
              return { ...p, isAudioEnabled: Boolean(audioEnabled) };
            }
            return p;
          });
          console.log('Updated participants:', updated);
          return updated;
        });
      });

      voiceCallApi.on('newProducer', async (producerData) => {
        console.log('New producer event received:', producerData);
        if (handleNewProducerRef.current) {
          await handleNewProducerRef.current(producerData);
        }
      });

      voiceCallApi.on('producerClosed', (data) => {
        console.log('Producer closed:', data);
        const producerId = data.producerId || data;
        const producerSocketId = data.producerSocketId;
        
        // Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ consumer
        const consumer = consumersRef.current.get(producerId);
        if (consumer) {
          consumer.close();
          consumersRef.current.delete(producerId);
        }
        
        // Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ producerSocketId, Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ userId Ð¸Ð· Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³Ð°
        if (producerSocketId) {
          const userId = peerIdToUserIdMapRef.current.get(producerSocketId);
          console.log('Producer closed for socketId:', producerSocketId, 'userId:', userId);
          
          if (userId) {
            // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ audio element
            const audioElement = audioElementsRef.current.get(userId);
            if (audioElement) {
              console.log('Removing audio element for user:', userId);
              audioElement.pause();
              audioElement.srcObject = null;
              if (audioElement.parentNode) {
                audioElement.parentNode.removeChild(audioElement);
              }
              audioElementsRef.current.delete(userId);
            }
            
            // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ gain node
            const gainNode = gainNodesRef.current.get(userId);
            if (gainNode) {
              console.log('Disconnecting gain node for user:', userId);
              try {
                gainNode.disconnect();
              } catch (e) {
                console.warn('Error disconnecting gain node:', e);
              }
              gainNodesRef.current.delete(userId);
            }
            
            // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
            setUserVolumes(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            setUserMutedStates(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            setShowVolumeSliders(prev => {
              const newMap = new Map(prev);
              newMap.delete(userId);
              return newMap;
            });
            
            previousVolumesRef.current.delete(userId);
            console.log('Audio cleanup completed for userId:', userId);
          }
        }
      });
      
      setIsConnected(true);
      connectingRef.current = false;
      
      // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€
      if (voiceCallApi.socket) {
        voiceCallApi.socket.emit('muteState', { isMuted: isMuted });
        voiceCallApi.socket.emit('audioState', { isEnabled: !isGlobalAudioMuted });
        console.log('Initial states sent to server - muted:', isMuted, 'audio (headphones on):', !isGlobalAudioMuted);
      }
    } catch (error) {
      console.error('Failed to connect to voice server:', error);
      setError(error.message);
      connectingRef.current = false;
    }
  }, [userId, userName, isMuted, isGlobalAudioMuted]);

  // ÐžÑ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚ ÑÐµÑ€Ð²ÐµÑ€Ð°
  const disconnect = useCallback(async () => {
    try {
      console.log('Disconnecting from voice call...');
      
      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¸ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹
      voiceCallApi.off('peerJoined');
      voiceCallApi.off('peerLeft');
      voiceCallApi.off('peerMuteStateChanged');
      voiceCallApi.off('peerAudioStateChanged');
      voiceCallApi.off('newProducer');
      voiceCallApi.off('producerClosed');
      console.log('Event handlers cleared');
      
      if (localStreamRef.current) {
        localStreamRef.current.getTracks().forEach(track => track.stop());
        localStreamRef.current = null;
      }
      
      // ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
      if (noiseSuppressionRef.current) {
        noiseSuppressionRef.current.cleanup();
        noiseSuppressionRef.current = null;
      }
      
      // Ð—Ð°ÐºÑ€Ñ‹Ñ‚Ð¸Ðµ audio context
      if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
        await audioContextRef.current.close();
        audioContextRef.current = null;
      }
      
      if (sendTransportRef.current) {
        sendTransportRef.current.close();
        sendTransportRef.current = null;
      }
      
      if (recvTransportRef.current) {
        recvTransportRef.current.close();
        recvTransportRef.current = null;
      }
      
      consumersRef.current.forEach(consumer => consumer.close());
      consumersRef.current.clear();
      
      producersRef.current.forEach(producer => producer.close());
      producersRef.current.clear();
      
      // ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° GainNodes Ð¸ audio elements
      gainNodesRef.current.forEach(gainNode => {
        try {
          gainNode.disconnect();
        } catch (e) {
          console.warn('Error disconnecting gain node:', e);
        }
      });
      gainNodesRef.current.clear();
      
      audioElementsRef.current.forEach(audioElement => {
        try {
          audioElement.pause();
          audioElement.srcObject = null;
          if (audioElement.parentNode) {
            audioElement.parentNode.removeChild(audioElement);
          }
        } catch (e) {
          console.warn('Error removing audio element:', e);
        }
      });
      audioElementsRef.current.clear();
      previousVolumesRef.current.clear();
      peerIdToUserIdMapRef.current.clear();
      
      await voiceCallApi.disconnect();
      setIsConnected(false);
      setCurrentCall(null); // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð·Ð²Ð¾Ð½Ð¾Ðº
      setParticipants([]);
      setUserVolumes(new Map());
      setUserMutedStates(new Map());
      setShowVolumeSliders(new Map());
      connectingRef.current = false;
      console.log('Disconnected from voice server');
    } catch (error) {
      console.error('Failed to disconnect:', error);
      connectingRef.current = false;
    }
  }, []);

  // Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ñ€Ð°Ð½ÑÐ¿Ð¾Ñ€Ñ‚Ð¾Ð²
  const createTransports = useCallback(async () => {
    try {
      // Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ send transport
      const sendTransportData = await voiceCallApi.createWebRtcTransport();
      sendTransportRef.current = deviceRef.current.createSendTransport({
        id: sendTransportData.id,
        iceParameters: sendTransportData.iceParameters,
        iceCandidates: sendTransportData.iceCandidates,
        dtlsParameters: sendTransportData.dtlsParameters,
        iceServers: ICE_SERVERS
      });

      sendTransportRef.current.on('connect', async ({ dtlsParameters }, callback, errback) => {
        try {
          await voiceCallApi.connectTransport(sendTransportData.id, dtlsParameters);
          callback();
        } catch (error) {
          errback(error);
        }
      });

      sendTransportRef.current.on('produce', async ({ kind, rtpParameters, appData }, callback, errback) => {
        try {
          const { id } = await voiceCallApi.produce(sendTransportData.id, kind, rtpParameters, appData);
          callback({ id });
        } catch (error) {
          errback(error);
        }
      });

      // Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ recv transport
      const recvTransportData = await voiceCallApi.createWebRtcTransport();
      recvTransportRef.current = deviceRef.current.createRecvTransport({
        id: recvTransportData.id,
        iceParameters: recvTransportData.iceParameters,
        iceCandidates: recvTransportData.iceCandidates,
        dtlsParameters: recvTransportData.dtlsParameters,
        iceServers: ICE_SERVERS
      });

      recvTransportRef.current.on('connect', async ({ dtlsParameters }, callback, errback) => {
        try {
          await voiceCallApi.connectTransport(recvTransportData.id, dtlsParameters);
          callback();
        } catch (error) {
          errback(error);
        }
      });

      console.log('Transports created');
    } catch (error) {
      console.error('Failed to create transports:', error);
      setError(error.message);
    }
  }, []);

  // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð°
  const initializeDevice = useCallback(async (routerRtpCapabilities) => {
    try {
      deviceRef.current = await voiceCallApi.initializeDevice(routerRtpCapabilities);
      await createTransports();
    } catch (error) {
      console.error('Failed to initialize device:', error);
      setError(error.message);
    }
  }, [createTransports]);

  // ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð½Ð¾Ð²Ð¾Ð³Ð¾ producer
  const handleNewProducer = useCallback(async (producerData) => {
    try {
      const consumerData = await voiceCallApi.consume(
        deviceRef.current.rtpCapabilities,
        producerData.producerId,
        recvTransportRef.current.id
      );

      const consumer = await recvTransportRef.current.consume({
        id: consumerData.id,
        producerId: producerData.producerId,
        kind: producerData.kind,
        rtpParameters: consumerData.rtpParameters
      });

      consumersRef.current.set(consumerData.id, consumer);
      
      // Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ userId Ð²Ð¼ÐµÑÑ‚Ð¾ producerSocketId Ð´Ð»Ñ ÐºÐ»ÑŽÑ‡ÐµÐ¹
      const socketId = producerData.producerSocketId;
      const userId = peerIdToUserIdMapRef.current.get(socketId) || socketId;
      console.log('handleNewProducer: socketId=', socketId, 'userId=', userId);
      
      // ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ ÑÑ‚Ð¾ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸ÐµÐ¹ ÑÐºÑ€Ð°Ð½Ð°
      const isScreenShare = producerData.appData?.mediaType === 'screen';
      console.log('handleNewProducer: isScreenShare=', isScreenShare, 'kind=', producerData.kind);
      
      // Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð° Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð¸Ð´ÐµÐ¾ (Ð°ÑƒÐ´Ð¸Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ÑÑ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾)
      if (isScreenShare && producerData.kind === 'video') {
        console.log('Screen share video producer detected, skipping audio processing');
        return;
      }
      
      // Ð”Ð»Ñ Ð°ÑƒÐ´Ð¸Ð¾ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð° ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ AudioContext
      if (isScreenShare && producerData.kind === 'audio') {
        console.log('Screen share audio producer detected, creating separate audio processing');
        console.log('Screen share audio producer data:', producerData);
        
        // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ð¹ AudioContext Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
        if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
          audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 48000,
            latencyHint: 'playback' // Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ playback
          });
        }
        
        // Resume audio context if suspended
        if (audioContextRef.current.state === 'suspended') {
          await audioContextRef.current.resume();
        }
        
        // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ audio element Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
        const audioElement = document.createElement('audio');
        audioElement.srcObject = new MediaStream([consumer.track]);
        audioElement.autoplay = true;
        audioElement.volume = 1.0; // ÐŸÐ¾Ð»Ð½Ð°Ñ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
        audioElement.muted = false;
        
        // Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð² DOM Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ
        document.body.appendChild(audioElement);
        
        // Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ audio element Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
        const screenShareAudioKey = `screen-share-${userId}`;
        audioElementsRef.current.set(screenShareAudioKey, audioElement);
        
        console.log('Screen share audio consumer created:', consumerData.id);
        return;
      }
      
      // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ AudioContext ÐµÑÐ»Ð¸ ÐµÑ‰Ðµ Ð½Ðµ ÑÐ¾Ð·Ð´Ð°Ð½ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð°ÑƒÐ´Ð¸Ð¾)
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 48000,
          latencyHint: 'interactive'
        });
      }
      
      // Resume audio context if suspended
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
      }
      
      // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ audio element
      const audioElement = document.createElement('audio');
      audioElement.srcObject = new MediaStream([consumer.track]);
      audioElement.autoplay = true;
      audioElement.playsInline = true;
      audioElement.controls = false;
      audioElement.style.display = 'none';
      document.body.appendChild(audioElement);
      
      // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Web Audio API chain: source -> gain
      // ÐÐ• Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ðº destination, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¸Ð·Ð±ÐµÐ¶Ð°Ñ‚ÑŒ Ð´Ð²Ð¾Ð¹Ð½Ð¾Ð³Ð¾ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ!
      // Ð’Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ðµ Ð¸Ð´ÐµÑ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‡ÐµÑ€ÐµÐ· HTML Audio ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
      const source = audioContextRef.current.createMediaStreamSource(new MediaStream([consumer.track]));
      const gainNode = audioContextRef.current.createGain();
      
      // GainNode Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ, Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ
      // Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ HTML Audio ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°
      const initialVolume = userVolumes.get(userId) || 100;
      const isMuted = userMutedStates.get(userId) || false;
      // Ð•ÑÐ»Ð¸ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾ Ð²Ñ‹ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð·Ð²ÑƒÐº, ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ 0, Ð¸Ð½Ð°Ñ‡Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½ÑƒÑŽ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ
      const audioVolume = isGlobalAudioMuted ? 0 : (isMuted ? 0 : (initialVolume / 100.0));
      audioElement.volume = audioVolume;
      
      // ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ source -> gain, Ð½Ð¾ ÐÐ• Ðº destination (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð°)
      source.connect(gainNode);
      // gainNode.connect(audioContextRef.current.destination); // ÐžÐ¢ÐšÐ›Ð®Ð§Ð•ÐÐž - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ HTML Audio
      
      // Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÑÑÑ‹Ð»ÐºÐ¸
      gainNodesRef.current.set(userId, gainNode);
      audioElementsRef.current.set(userId, audioElement);
      
      // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒ Ð² ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ð¸ ÐµÑÐ»Ð¸ ÐµÑ‰Ðµ Ð½Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð°
      if (!userVolumes.has(userId)) {
        setUserVolumes(prev => {
          const newMap = new Map(prev);
          newMap.set(userId, 100);
          return newMap;
        });
      }

      try {
        await audioElement.play();
        console.log('Audio playback started for peer:', userId);
        setAudioBlocked(false);
      } catch (error) {
        console.log('Auto-play blocked, user interaction required:', error);
        setAudioBlocked(true);
        setTimeout(async () => {
          try {
            await audioElement.play();
            setAudioBlocked(false);
          } catch {
            console.log('Audio playback still blocked');
          }
        }, 1000);
      }

      await voiceCallApi.resumeConsumer(consumerData.id);
      console.log('New consumer created:', consumerData.id);
    } catch (error) {
      console.error('Failed to handle new producer:', error);
    }
  }, [userVolumes, userMutedStates, isGlobalAudioMuted]);
  
  // ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ref Ð¿Ñ€Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¸ handleNewProducer
  handleNewProducerRef.current = handleNewProducer;

  // Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð°ÑƒÐ´Ð¸Ð¾ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°
  const createAudioStream = useCallback(async () => {
    console.log('createAudioStream called with:', { isMuted, isNoiseSuppressed, noiseSuppressionMode });
    try {
      console.log('Creating audio stream...');
      
      // Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ producers Ð¿ÐµÑ€ÐµÐ´ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸ÐµÐ¼ Ð½Ð¾Ð²Ð¾Ð³Ð¾
      if (producersRef.current.size > 0) {
        console.log('Closing existing producers:', producersRef.current.size);
        producersRef.current.forEach(producer => {
          try {
            producer.close();
          } catch (e) {
            console.warn('Error closing producer:', e);
          }
        });
        producersRef.current.clear();
      }
      
      // ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ðº ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
      if (localStreamRef.current) {
        console.log('Stopping existing local stream');
        localStreamRef.current.getTracks().forEach(track => track.stop());
      }
      
      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ€Ð¾Ðµ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ
      if (noiseSuppressionRef.current) {
        console.log('Cleaning up existing noise suppression');
        noiseSuppressionRef.current.cleanup();
      }
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
          channelCount: 1,
          latency: 0,
          suppressLocalAudioPlayback: true
        }
      });

      localStreamRef.current = stream;
      console.log('Got user media stream');
      
      // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ audio context
      if (!audioContextRef.current || audioContextRef.current.state === 'closed') {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
          sampleRate: 48000,
          latencyHint: 'interactive'
        });
        console.log('Created new AudioContext');
      }
      
      // Resume audio context if suspended
      if (audioContextRef.current.state === 'suspended') {
        await audioContextRef.current.resume();
        console.log('Resumed AudioContext');
      }
      
      // Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
      noiseSuppressionRef.current = new NoiseSuppressionManager();
      await noiseSuppressionRef.current.initialize(stream, audioContextRef.current);
      
      // ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ð¾Ñ‚Ð¾Ðº
      const processedStream = noiseSuppressionRef.current.getProcessedStream();
      const audioTrack = processedStream.getAudioTracks()[0];
      
      if (!audioTrack) {
        throw new Error('No audio track in processed stream');
      }
      
      // ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ ÐµÑÐ»Ð¸ Ð¾Ð½Ð¾ Ð±Ñ‹Ð»Ð¾ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾
      if (isNoiseSuppressed) {
        console.log('Applying noise suppression:', noiseSuppressionMode);
        const enabled = await noiseSuppressionRef.current.enable(noiseSuppressionMode);
        if (!enabled) {
          console.warn('Failed to enable noise suppression, continuing without it');
        }
      }
      
      // Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°
      audioTrack.enabled = !isMuted;
      console.log('Audio track muted state:', !audioTrack.enabled);
      
      const producer = await sendTransportRef.current.produce({
        track: audioTrack,
        appData: { userId, userName }
      });

      producersRef.current.set(producer.id, producer);
      console.log('Producer created with ID:', producer.id);
      
      // Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ producer Ð² noise suppression manager
      if (noiseSuppressionRef.current) {
        noiseSuppressionRef.current.setProducer(producer);
      }
      
      audioTrack.onended = () => {
        console.log('Audio track ended');
      };

      console.log('Audio stream created with noise suppression support');
      return producer;
    } catch (error) {
      console.error('Failed to create audio stream:', error);
      setError(error.message);
    }
  }, [userId, userName, isMuted, isNoiseSuppressed, noiseSuppressionMode]);

  // ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ref Ð¿Ñ€Ð¸ Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ð¸ createAudioStream
  createAudioStreamRef.current = createAudioStream;

  // ÐŸÑ€Ð¸ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ Ðº ÐºÐ¾Ð¼Ð½Ð°Ñ‚Ðµ
  const joinRoom = useCallback(async (roomId) => {
    try {
      console.log('joinRoom called for roomId:', roomId);
      console.trace('joinRoom call stack');
      const response = await voiceCallApi.joinRoom(roomId, userName, userId);
      
      if (response.routerRtpCapabilities) {
        await initializeDevice(response.routerRtpCapabilities);
      }
      
      if (response.existingPeers) {
        // Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¼Ð°Ð¿Ð¿Ð¸Ð½Ð³ Ð´Ð»Ñ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¸Ñ€Ð¾Ð²
        response.existingPeers.forEach(peer => {
          const socketId = peer.peerId || peer.id;
          if (socketId && peer.userId) {
            peerIdToUserIdMapRef.current.set(socketId, peer.userId);
            console.log('Updated existing peer mapping:', socketId, '->', peer.userId);
          }
        });
        
        console.log('All peer mappings after loading existing peers:', Array.from(peerIdToUserIdMapRef.current.entries()));
        
        setParticipants(response.existingPeers.map(peer => ({
          userId: peer.userId,
          peerId: peer.peerId || peer.id,
          name: peer.name,
          isMuted: peer.isMuted || false,
          isAudioEnabled: peer.isAudioEnabled !== undefined ? peer.isAudioEnabled : true,
          isSpeaking: false
        })));
      }
      
      if (response.existingProducers && response.existingProducers.length > 0) {
        console.log('Processing existing producers:', response.existingProducers);
        for (const producer of response.existingProducers) {
          try {
            await handleNewProducer(producer);
          } catch (error) {
            console.error('Failed to process existing producer:', error);
          }
        }
      }
      
      // Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ref Ð²Ð¼ÐµÑÑ‚Ð¾ Ð¿Ñ€ÑÐ¼Ð¾Ð³Ð¾ Ð²Ñ‹Ð·Ð¾Ð²Ð°
      if (createAudioStreamRef.current) {
        await createAudioStreamRef.current();
      }
      
      // ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹ Ð·Ð²Ð¾Ð½Ð¾Ðº
      setCurrentCall({ channelId: roomId, channelName: roomId });
      console.log('Current call set to:', { channelId: roomId, channelName: roomId });
    } catch (error) {
      console.error('Failed to join room:', error);
      setError(error.message);
    }
  }, [userName, userId, initializeDevice, handleNewProducer]); // Ð£Ð±Ñ€Ð°Ð»Ð¸ createAudioStream Ð¸Ð· Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚ÐµÐ¹

  // ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð°
  const toggleMute = useCallback(() => {
    const newMutedState = !isMuted;
    
    if (noiseSuppressionRef.current) {
      const processedStream = noiseSuppressionRef.current.getProcessedStream();
      const audioTrack = processedStream?.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !newMutedState;
        setIsMuted(newMutedState);
      }
    } else if (localStreamRef.current) {
      const audioTrack = localStreamRef.current.getAudioTracks()[0];
      if (audioTrack) {
        audioTrack.enabled = !newMutedState;
        setIsMuted(newMutedState);
      }
    }
    
    // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÑƒÑ‚Ð° Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€
    if (voiceCallApi.socket) {
      voiceCallApi.socket.emit('muteState', { isMuted: newMutedState });
      console.log('Mute state sent to server:', newMutedState);
    }
  }, [isMuted]);


  // Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸
  const handleVolumeChange = useCallback((newVolume) => {
    setVolume(newVolume);
    const audioElements = document.querySelectorAll('audio');
    audioElements.forEach(audio => {
      audio.volume = newVolume;
    });
  }, []);

  // ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¼ÑƒÑ‚Ð° Ð´Ð»Ñ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const toggleUserMute = useCallback((peerId) => {
    console.log('toggleUserMute called for:', peerId);
    console.log('Available audio elements:', Array.from(audioElementsRef.current.keys()));
    const audioElement = audioElementsRef.current.get(peerId);
    if (!audioElement) {
      console.error('Audio element not found for peer:', peerId);
      return;
    }

    const isCurrentlyMuted = userMutedStates.get(peerId) || false;
    const newIsMuted = !isCurrentlyMuted;

    if (newIsMuted) {
      // ÐœÑƒÑ‚Ð¸Ð¼ - ÑƒÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ 0, ÐÐ• Ð¼ÐµÐ½ÑÑ Ð¿Ð¾Ð»Ð·ÑƒÐ½Ð¾Ðº (ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ñ‚ÐµÐºÑƒÑ‰ÐµÐµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ)
      const currentVolume = userVolumes.get(peerId) || 100;
      previousVolumesRef.current.set(peerId, currentVolume);
      audioElement.volume = 0;
      // ÐÐ• Ð¼ÐµÐ½ÑÐµÐ¼ userVolumes, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð»Ð·ÑƒÐ½Ð¾Ðº Ð¾ÑÑ‚Ð°Ð»ÑÑ Ð½Ð° Ð¼ÐµÑÑ‚Ðµ
    } else {
      // Ð Ð°Ð·Ð¼ÑƒÑ‚Ð¸Ð²Ð°ÐµÐ¼ - Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð·Ð²ÑƒÐº Ð½Ð° Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ Ð¿Ð¾Ð·Ð¸Ñ†Ð¸ÑŽ Ð¿Ð¾Ð»Ð·ÑƒÐ½ÐºÐ°
      const currentVolume = userVolumes.get(peerId) || 100;
      const audioVolume = isGlobalAudioMuted ? 0 : (currentVolume / 100.0);
      audioElement.volume = audioVolume;
      // ÐÐ• Ð¼ÐµÐ½ÑÐµÐ¼ userVolumes, Ð¿Ð¾Ð»Ð·ÑƒÐ½Ð¾Ðº ÑƒÐ¶Ðµ Ð½Ð° Ð½ÑƒÐ¶Ð½Ð¾Ð¼ Ð¼ÐµÑÑ‚Ðµ
    }

    setUserMutedStates(prev => {
      const newMap = new Map(prev);
      newMap.set(peerId, newIsMuted);
      return newMap;
    });

    console.log(`User ${peerId} ${newIsMuted ? 'muted' : 'unmuted'}`);
  }, [userVolumes, userMutedStates, isGlobalAudioMuted]);

  // Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
  const changeUserVolume = useCallback((peerId, newVolume) => {
    console.log('changeUserVolume called for:', peerId, 'newVolume:', newVolume);
    console.log('Available audio elements:', Array.from(audioElementsRef.current.keys()));
    const audioElement = audioElementsRef.current.get(peerId);
    if (!audioElement) {
      console.error('Audio element not found for peer:', peerId);
      return;
    }

    // Ð•ÑÐ»Ð¸ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾ Ð·Ð°Ð¼ÑƒÑ‡ÐµÐ½Ð¾, Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ, Ð½Ð¾ Ð½Ðµ Ð·Ð²ÑƒÐº
    const audioVolume = isGlobalAudioMuted ? 0 : (newVolume / 100.0);
    audioElement.volume = audioVolume;

    setUserVolumes(prev => {
      const newMap = new Map(prev);
      newMap.set(peerId, newVolume);
      return newMap;
    });

    // Ð•ÑÐ»Ð¸ Ñ€Ð°Ð·Ð¼ÑƒÑ‚Ð¸Ð²Ð°ÐµÐ¼ Ñ‡ÐµÑ€ÐµÐ· ÑÐ»Ð°Ð¹Ð´ÐµÑ€, Ð¾Ð±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼ÑƒÑ‚Ð°
    if (newVolume > 0 && userMutedStates.get(peerId)) {
      setUserMutedStates(prev => {
        const newMap = new Map(prev);
        newMap.set(peerId, false);
        return newMap;
      });
    } else if (newVolume === 0 && !userMutedStates.get(peerId)) {
      setUserMutedStates(prev => {
        const newMap = new Map(prev);
        newMap.set(peerId, true);
        return newMap;
      });
    }

    console.log(`User ${peerId} volume set to ${newVolume}%`);
  }, [userMutedStates, isGlobalAudioMuted]);

  // ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ ÑÐ»Ð°Ð¹Ð´ÐµÑ€Ð° Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚Ð¸
  const toggleVolumeSlider = useCallback((peerId) => {
    setShowVolumeSliders(prev => {
      const newMap = new Map(prev);
      const currentState = newMap.get(peerId) || false;
      newMap.set(peerId, !currentState);
      return newMap;
    });
  }, []);

  // Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ/Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð·Ð²ÑƒÐºÐ° Ð²ÑÐµÑ… ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð² (Ð½Ð°ÑƒÑˆÐ½Ð¸ÐºÐ¸)
  const toggleGlobalAudio = useCallback(() => {
    const newMutedState = !isGlobalAudioMuted;
    
    console.log(`toggleGlobalAudio called, new state: ${newMutedState}`);
    console.log(`Audio elements count: ${audioElementsRef.current.size}`);
    
    // ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð½Ð°ÑƒÑˆÐ½Ð¸ÐºÐ¾Ð² Ð½Ð° ÑÐµÑ€Ð²ÐµÑ€
    // isGlobalAudioMuted=true Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ isAudioEnabled=false
    if (voiceCallApi.socket) {
      voiceCallApi.socket.emit('audioState', { isEnabled: !newMutedState });
      console.log('Audio state (headphones) sent to server, isEnabled:', !newMutedState);
    }
    
    // Ð£Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ HTML Audio ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ (GainNode Ð±Ð¾Ð»ÑŒÑˆÐµ Ð½Ðµ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð´Ð»Ñ Ð²Ð¾ÑÐ¿Ñ€Ð¾Ð¸Ð·Ð²ÐµÐ´ÐµÐ½Ð¸Ñ)
    audioElementsRef.current.forEach((audioElement, peerId) => {
      if (audioElement) {
        if (newMutedState) {
          // ÐœÑƒÑ‚Ð¸Ð¼ HTML Audio ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚
          audioElement.volume = 0;
          console.log(`HTML Audio muted for peer: ${peerId}`);
        } else {
          // Ð Ð°Ð·Ð¼ÑƒÑ‚Ð¸Ð²Ð°ÐµÐ¼ Ñ Ð¸Ð½Ð´Ð¸Ð²Ð¸Ð´ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð³Ñ€Ð¾Ð¼ÐºÐ¾ÑÑ‚ÑŒÑŽ
          const volume = userVolumes.get(peerId) || 100;
          const isIndividuallyMuted = userMutedStates.get(peerId) || false;
          const audioVolume = isIndividuallyMuted ? 0 : (volume / 100.0);
          audioElement.volume = audioVolume;
          console.log(`HTML Audio unmuted for peer: ${peerId}, volume: ${audioVolume}`);
        }
      }
    });
    
    setIsGlobalAudioMuted(newMutedState);
    console.log(`Global audio ${newMutedState ? 'muted' : 'unmuted'}`);
  }, [isGlobalAudioMuted, userVolumes, userMutedStates]);

  // ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
  const toggleNoiseSuppression = useCallback(async () => {
    try {
      if (!noiseSuppressionRef.current || !noiseSuppressionRef.current.isInitialized()) {
        console.error('Noise suppression not initialized');
        return false;
      }

      const newState = !isNoiseSuppressed;
      console.log(`Toggling noise suppression: ${isNoiseSuppressed} -> ${newState}`);
      
      let success = false;

      if (newState) {
        success = await noiseSuppressionRef.current.enable(noiseSuppressionMode);
        console.log(`Noise suppression enabled with mode: ${noiseSuppressionMode}, success: ${success}`);
      } else {
        success = await noiseSuppressionRef.current.disable();
        console.log(`Noise suppression disabled, success: ${success}`);
      }

      if (success) {
        setIsNoiseSuppressed(newState);
        localStorage.setItem('noiseSuppression', JSON.stringify(newState));
        
        // ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ‚Ñ€ÐµÐºÐ°
        if (noiseSuppressionRef.current) {
          const processedStream = noiseSuppressionRef.current.getProcessedStream();
          const audioTrack = processedStream?.getAudioTracks()[0];
          if (audioTrack) {
            audioTrack.enabled = !isMuted;
            console.log(`Updated audio track enabled state: ${audioTrack.enabled}`);
          }
        }
        
        console.log('Noise suppression ' + (newState ? 'enabled' : 'disabled') + ' successfully');
        return true;
      } else {
        console.error('Failed to toggle noise suppression');
        return false;
      }
    } catch (error) {
      console.error('Error toggling noise suppression:', error);
      return false;
    }
  }, [isNoiseSuppressed, noiseSuppressionMode, isMuted]);

  // Ð˜Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ðµ Ñ€ÐµÐ¶Ð¸Ð¼Ð° ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ
  const changeNoiseSuppressionMode = useCallback(async (mode) => {
    try {
      if (!noiseSuppressionRef.current || !noiseSuppressionRef.current.isInitialized()) {
        console.error('Noise suppression not initialized');
        return false;
      }

      console.log(`Changing noise suppression mode from ${noiseSuppressionMode} to ${mode}`);
      
      let success = false;

      if (!isNoiseSuppressed) {
        success = await noiseSuppressionRef.current.enable(mode);
        console.log(`Enabled noise suppression with mode: ${mode}, success: ${success}`);
      } else if (mode !== noiseSuppressionMode) {
        // Ð•ÑÐ»Ð¸ Ð¼ÐµÐ½ÑÐµÐ¼ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¿Ñ€Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ð¾Ð¼ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸
        success = await noiseSuppressionRef.current.enable(mode);
        console.log(`Changed mode to: ${mode}, success: ${success}`);
      } else {
        console.log(`Mode ${mode} is already active`);
        return true; // Ð ÐµÐ¶Ð¸Ð¼ ÑƒÐ¶Ðµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½
      }

      if (success) {
        setNoiseSuppressionMode(mode);
        setIsNoiseSuppressed(true);
        localStorage.setItem('noiseSuppression', JSON.stringify(true));
        
        // ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ Ð¼Ð¸ÐºÑ€Ð¾Ñ„Ð¾Ð½Ð° Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½Ð½Ð¾Ð³Ð¾ Ñ‚Ñ€ÐµÐºÐ°
        if (noiseSuppressionRef.current) {
          const processedStream = noiseSuppressionRef.current.getProcessedStream();
          const audioTrack = processedStream?.getAudioTracks()[0];
          if (audioTrack) {
            audioTrack.enabled = !isMuted;
            console.log(`Updated audio track enabled state after mode change: ${audioTrack.enabled}`);
          }
        }
        
        console.log('Noise suppression mode changed to:', mode);
        return true;
      }
      
      return false;
    } catch (error) {
      console.error('Error changing noise suppression mode:', error);
      return false;
    }
  }, [isNoiseSuppressed, noiseSuppressionMode, isMuted]);

  // ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸Ðº Ð¸Ð·Ð¼ÐµÐ½ÐµÐ½Ð¸Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð¸Ð· Ð´Ñ€ÑƒÐ³Ð¸Ñ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð²
  useEffect(() => {
    const handleNoiseSuppressionChanged = (event) => {
      const { enabled } = event.detail;
      setIsNoiseSuppressed(enabled);
      
      // Ð•ÑÐ»Ð¸ ÑˆÑƒÐ¼Ð¾Ð¿Ð¾Ð´Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¾ Ð¸ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ðº, Ð²ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ ÐµÐ³Ð¾
      if (enabled && noiseSuppressionRef.current) {
        noiseSuppressionRef.current.enable(noiseSuppressionMode);
      } else if (!enabled && noiseSuppressionRef.current) {
        noiseSuppressionRef.current.disable();
      }
    };

    window.addEventListener('noiseSuppressionChanged', handleNoiseSuppressionChanged);
    return () => {
      window.removeEventListener('noiseSuppressionChanged', handleNoiseSuppressionChanged);
    };
  }, [noiseSuppressionMode]);

  // Ð”ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÐºÑ€Ð°Ð½Ð°
    const startScreenShare = useCallback(async () => {
      try {
        console.log('ðŸš€ðŸš€ðŸš€ STARTING SCREEN SHARE FUNCTION CALLED ðŸš€ðŸš€ðŸš€');
        
        if (!sendTransportRef.current) {
          throw new Error('Transport not ready');
        }

        // ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸ÑŽ ÑÐºÑ€Ð°Ð½Ð°, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
        if (isScreenSharing) {
          await stopScreenShare();
        }

        console.log('=== STARTING SCREEN SHARE ===');
      console.log('Requesting screen sharing access...');
      const stream = await navigator.mediaDevices.getDisplayMedia({
        video: {
          cursor: 'always',
          frameRate: { ideal: 60, max: 60 },
          width: { ideal: 1920, max: 1920 },
          height: { ideal: 1080, max: 1080 },
          aspectRatio: 16/9,
          displaySurface: 'monitor',
          resizeMode: 'crop-and-scale'
        },
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 48000,
          channelCount: 2,
          sampleSize: 16
        }
      });

      console.log('Screen sharing access granted');

      // ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐ¸ Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼
      stream.getVideoTracks()[0].onended = () => {
        console.log('Screen sharing stopped by user');
        stopScreenShare();
      };

      // Ð£ÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ñ‚Ð¾Ðº
      setScreenShareStream(stream);

      const videoTrack = stream.getVideoTracks()[0];
      const audioTrack = stream.getAudioTracks()[0];
      
      console.log('Stream tracks:', {
        videoTracks: stream.getVideoTracks().length,
        audioTracks: stream.getAudioTracks().length,
        videoTrack: !!videoTrack,
        audioTrack: !!audioTrack
      });
      
      if (!videoTrack) {
        throw new Error('No video track available');
      }

      console.log('Creating screen sharing producers...');
      console.log('Video track:', videoTrack);
      console.log('Audio track:', audioTrack);
      
      // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ video producer Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
      console.log('Creating video producer...');
      const videoProducer = await sendTransportRef.current.produce({
        track: videoTrack,
        encodings: [
          {
            scaleResolutionDownBy: 1,
            maxBitrate: 5000000, // 5 Mbps Ð´Ð»Ñ Full HD
            maxFramerate: 60
          }
        ],
        codecOptions: {
          videoGoogleStartBitrate: 3000, // ÐÐ°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±Ð¸Ñ‚Ñ€ÐµÐ¹Ñ‚ 3 Mbps
          videoGoogleMaxBitrate: 5000 // ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð±Ð¸Ñ‚Ñ€ÐµÐ¹Ñ‚ 5 Mbps
        },
        appData: {
          mediaType: 'screen',
          trackType: 'video',
          userId: userId,
          userName: userName,
          width: videoTrack.getSettings().width,
          height: videoTrack.getSettings().height,
          frameRate: videoTrack.getSettings().frameRate
        }
      });

      console.log('Screen sharing video producer created:', videoProducer.id);

      // Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ audio producer Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð°ÑƒÐ´Ð¸Ð¾ Ñ‚Ñ€ÐµÐº
      let audioProducer = null;
      if (audioTrack) {
        console.log('Creating audio producer...');
        audioProducer = await sendTransportRef.current.produce({
          track: audioTrack,
          encodings: [
            {
              ssrc: Math.floor(Math.random() * 4294967296),
              dtx: true,
              maxBitrate: 128000, // 128 kbps Ð´Ð»Ñ Ð°ÑƒÐ´Ð¸Ð¾ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
              scalabilityMode: 'S1T1',
              numberOfChannels: 2
            }
          ],
          codecOptions: {
            opusStereo: true,
            opusDtx: true,
            opusFec: true,
            opusNack: true,
            channelsCount: 2,
            sampleRate: 48000,
            opusMaxAverageBitrate: 128000,
            opusMaxPlaybackRate: 48000,
            opusPtime: 20,
            opusApplication: 'music', // Ð”Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ music Ð²Ð¼ÐµÑÑ‚Ð¾ voip
            opusCbr: false,
            opusUseinbandfec: true
          },
          appData: {
            mediaType: 'screen',
            trackType: 'audio',
            userId: userId,
            userName: userName,
            audioProcessing: {
              echoCancellation: false, // ÐžÑ‚ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
              noiseSuppression: false,
              autoGainControl: false,
              highpassFilter: false,
              typingNoiseDetection: false,
              monoAudio: false
            }
          }
        });

        console.log('Screen sharing audio producer created:', audioProducer.id);
      } else {
        console.log('No audio track available for screen sharing');
      }

      // Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ producers
      screenProducerRef.current = { video: videoProducer, audio: audioProducer };
      console.log('Screen sharing producers saved:', { 
        video: videoProducer.id, 
        audio: audioProducer ? audioProducer.id : 'none' 
      });
      setIsScreenSharing(true);

      // ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ video producer
      videoProducer.on('transportclose', () => {
        console.log('Screen sharing video transport closed');
        stopScreenShare();
      });

      videoProducer.on('trackended', () => {
        console.log('Screen sharing video track ended');
        stopScreenShare();
      });

      // ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ audio producer, ÐµÑÐ»Ð¸ Ð¾Ð½ ÐµÑÑ‚ÑŒ
      if (audioProducer) {
        audioProducer.on('transportclose', () => {
          console.log('Screen sharing audio transport closed');
          stopScreenShare();
        });

        audioProducer.on('trackended', () => {
          console.log('Screen sharing audio track ended');
          stopScreenShare();
        });
      }

    } catch (error) {
      console.error('Error starting screen share:', error);
      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ
      if (screenShareStream) {
        screenShareStream.getTracks().forEach(track => track.stop());
      }
      setScreenShareStream(null);
      screenProducerRef.current = null;
      setIsScreenSharing(false);
      setError('Failed to start screen sharing: ' + error.message);
    }
  }, [userId, userName, isScreenSharing, screenShareStream, stopScreenShare]);

  const stopScreenShare = useCallback(async () => {
    console.log('Stopping screen sharing...');

    try {
      // Ð£Ð²ÐµÐ´Ð¾Ð¼Ð»ÑÐµÐ¼ ÑÐµÑ€Ð²ÐµÑ€ Ð¾Ð± Ð¾ÑÑ‚Ð°Ð½Ð¾Ð²ÐºÐµ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
      if (screenProducerRef.current && voiceCallApi.socket) {
        // ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ video producer
        if (screenProducerRef.current.video) {
          await voiceCallApi.stopScreenSharing(screenProducerRef.current.video.id);
        }
        // ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ audio producer, ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
        if (screenProducerRef.current.audio) {
          await voiceCallApi.stopScreenSharing(screenProducerRef.current.audio.id);
        }
      }

      // ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ð¾Ñ‚Ð¾Ðº
      if (screenShareStream) {
        screenShareStream.getTracks().forEach(track => track.stop());
      }

      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ audio elements Ð´Ð»Ñ Ð´ÐµÐ¼Ð¾Ð½ÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ ÑÐºÑ€Ð°Ð½Ð°
      const screenShareAudioKey = `screen-share-${userId}`;
      const screenShareAudioElement = audioElementsRef.current.get(screenShareAudioKey);
      if (screenShareAudioElement) {
        console.log('Removing screen share audio element for user:', userId);
        screenShareAudioElement.pause();
        screenShareAudioElement.srcObject = null;
        if (screenShareAudioElement.parentNode) {
          screenShareAudioElement.parentNode.removeChild(screenShareAudioElement);
        }
        audioElementsRef.current.delete(screenShareAudioKey);
      }

      // Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ audio elements Ð¿Ð¾ÑÐ»Ðµ Ð¾Ñ‡Ð¸ÑÑ‚ÐºÐ¸
      console.log('Audio elements after screen share cleanup:', Array.from(audioElementsRef.current.keys()));

      // ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
      setScreenShareStream(null);
      screenProducerRef.current = null;
      setIsScreenSharing(false);

      console.log('Screen sharing stopped successfully');
    } catch (error) {
      console.error('Error stopping screen share:', error);
      setError('Failed to stop screen sharing: ' + error.message);
    }
  }, [screenShareStream, userId]);

  const toggleScreenShare = useCallback(async () => {
    console.log('ðŸŽ¯ðŸŽ¯ðŸŽ¯ TOGGLE SCREEN SHARE CALLED ðŸŽ¯ðŸŽ¯ðŸŽ¯', { isScreenSharing });
    console.log('ðŸ”¥ðŸ”¥ðŸ”¥ TOGGLE FUNCTION EXECUTING ðŸ”¥ðŸ”¥ðŸ”¥');
    console.log('ðŸš€ðŸš€ðŸš€ TOGGLE FUNCTION START ðŸš€ðŸš€ðŸš€');
    console.log('ðŸ’¥ðŸ’¥ðŸ’¥ TOGGLE FUNCTION MIDDLE ðŸ’¥ðŸ’¥ðŸ’¥');
    console.log('ðŸŽªðŸŽªðŸŽª TOGGLE FUNCTION END ðŸŽªðŸŽªðŸŽª');
    if (isScreenSharing) {
      await stopScreenShare();
    } else {
      await startScreenShare();
    }
  }, [isScreenSharing, startScreenShare, stopScreenShare]);

  return {
    // Ð¡Ð¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ
    isConnected,
    isMuted,
    isAudioEnabled: !isGlobalAudioMuted, // Ð”Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸: isAudioEnabled Ð¾Ð·Ð½Ð°Ñ‡Ð°ÐµÑ‚ "Ð½Ð°ÑƒÑˆÐ½Ð¸ÐºÐ¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ñ‹"
    isSpeaking,
    participants,
    volume,
    audioBlocked,
    error,
    isNoiseSuppressed,
    noiseSuppressionMode,
    userVolumes,
    userMutedStates,
    showVolumeSliders,
    isGlobalAudioMuted,
    currentCall,
    isScreenSharing,
    screenShareStream,
    
    // ÐœÐµÑ‚Ð¾Ð´Ñ‹
    connect,
    disconnect,
    joinRoom,
    toggleMute,
    handleVolumeChange,
    toggleNoiseSuppression,
    changeNoiseSuppressionMode,
    toggleUserMute,
    changeUserVolume,
    toggleVolumeSlider,
    toggleGlobalAudio,
    startScreenShare,
    stopScreenShare,
    toggleScreenShare
  };
};